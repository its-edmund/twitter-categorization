{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Live Prediction Final Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mxinburritos/Disaster-Tweet-Sorting/blob/master/Live_Prediction_Final_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVhils5DlfU4",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing: Twitter Classification\n",
        "The problem being solved is being able to detect whether a tweet is relevant or irrelevant to a disaster. Many times, people who tweet make references to disaster to describe but aren't referring to a real disaster. \n",
        "\n",
        "DATASET: http://bit.ly/Twitter_Dataset\n",
        "\n",
        "Rather than use a neural network, we used a Naive Bayes algorithm, specifically Bernoulli which is a specialized form of logarithmic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqKLbsE0okKB",
        "colab_type": "text"
      },
      "source": [
        "#Twitter Training Dataset\n",
        "\n",
        "\n",
        "1.   Number of Instances: 7416 instances\n",
        "2.   Number of Attributes: 2 plus the class attribute\n",
        "3.   Attribute Information: Attribute Domain\n",
        "\n",
        "      1. Index of tweet\n",
        "      2. Class: 0 for irrelevant, 1 for relevant\n",
        "      3. Text: Text of tweet\n",
        "      \n",
        "4. Missing Attribute Value: N/A\n",
        "5. Class Distribution:\n",
        "\n",
        "  Instance belonging to class 0: 4,305 (58.1%)\n",
        "  \n",
        "  Instances belonging to class 1: 3,111 (41.9%)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9J3AO8kGL3q",
        "colab_type": "code",
        "outputId": "f6a2055e-7ba3-44a7-aa05-0c7d50fb6ad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Imports\n",
        "            \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sklearn\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras  \n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from sklearn.naive_bayes import BernoulliNB\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0JZ-gkvGetx",
        "colab_type": "code",
        "outputId": "f185791c-a125-409b-fe32-f05cc8768642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Data Collection\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "df2 = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Take relevant data from DataFrame and put it in Numpy Array\n",
        "\n",
        "tweets = df['text'].tolist()\n",
        "labels = df['class_label'].to_numpy()\n",
        "testTweets = df2['text'].tolist()\n",
        "testLabels = df2['class_label'].to_numpy()\n",
        "\n",
        "print(df.head())\n",
        "print(df.tail())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  class_label                                               text\n",
            "0        8525            0                       she keep it wet like tsunami\n",
            "1        5008            1  when ur friend and u are talking about forest ...\n",
            "2        8803            0  but i will be uploading these videos asap so y...\n",
            "3        6795            0              i'm interested   is it through yahoo?\n",
            "4        4603            0                   holy fuck someone set me on fire\n",
            "      Unnamed: 0  ...                                               text\n",
            "7411        7850  ...  and if your best evidence is the word of a guy...\n",
            "7412        3611  ...  i'm gonna drown myself in leftover chilis wish...\n",
            "7413        5969  ...                  i look like a mass murderer in it\n",
            "7414        5435  ...  who's that shadow holdin me hostage i've been ...\n",
            "7415        7618  ...  i liked a  video  boeing 737 takeoff in snowst...\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqHE0zBnrT-L",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing\n",
        "We created a lemmatizing method and defined a list of stopwords to remove. We use a vectorizer with the defined a preprocessing method and stopwords to convert all the tweets into a large vector. Each represents "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RTOf5-UqMRC",
        "colab_type": "code",
        "outputId": "62a3844e-b772-411f-e223-bb72e6330ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# TFID Vectorizer\n",
        "def preprocess(s):\n",
        "  lemmatizer = nltk.WordNetLemmatizer()\n",
        "  return lemmatizer.lemmatize(s)\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Stopwords part\n",
        "\n",
        "def not_stopword(s):\n",
        "  s = s.strip()\n",
        "  v = stopwords.words('english')\n",
        "  result = \"\"\n",
        "  words = nltk.word_tokenize(s)\n",
        "  for word in words:\n",
        "    if word not in v:\n",
        "      result += word + \" \"\n",
        "  return result.strip()\n",
        "\n",
        "i=0\n",
        "for token in tokens:\n",
        "  token = preprocess(token)\n",
        "  \n",
        "finalsentence = ' '.join(tweet.split())\n",
        "\n",
        "\n",
        "print(finalsentence)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=stop, analyzer='word', max_features=20000, dtype=np.float32, preprocessor=preprocess)\n",
        "\n",
        "data = vectorizer.fit_transform(tweets).toarray()\n",
        "testData = vectorizer.transform(testTweets).toarray()\n",
        "print(type(data), data)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'ha', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uihXxOg5Glv5",
        "colab_type": "code",
        "outputId": "6da15da1-7295-46b2-ca53-56f94b68ee93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.3, random_state=10)\n",
        "X_train = data\n",
        "Y_train = labels\n",
        "X_test = testData\n",
        "Y_test = testLabels\n",
        "\n",
        "print(len(X_test[0]))\n",
        "print(Y_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14839\n",
            "[1 1 1 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNHuU2fAGp8K",
        "colab_type": "code",
        "outputId": "f7baf4f9-4a8f-4200-e3ea-fd72da795273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "clf = BernoulliNB()\n",
        "clf.fit(X_train, Y_train)\n",
        "BernoulliNB(alpha=2.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
        "\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "print(clf.score(X_test, Y_test))\n",
        "\n",
        "print(recall_score(Y_test, predictions, average='macro'))\n",
        "print(precision_score(Y_test, predictions, average='macro'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 1 0 0]\n",
            "0.8058252427184466\n",
            "0.7808352143656005\n",
            "0.8184909531575053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5BoC0MZIHcI",
        "colab_type": "code",
        "outputId": "f1d3b28e-30f0-4601-b735-2ca18e254480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# live predictor\n",
        "keepGoing = True\n",
        "while keepGoing: \n",
        "  liveTester = input(\"Enter a tweet to see if it's relevant to a disaster: \\n\")\n",
        "\n",
        "  \n",
        "  #print(liveTester)\n",
        "\n",
        "  liveData = vectorizer.transform([liveTester]).toarray()\n",
        "\n",
        "  #print(liveData)\n",
        "  \n",
        "  ans = clf.predict(liveData)\n",
        "\n",
        "  if ans == 0:\n",
        "    print(\"\\nTweet not relevant to a disaster\\n\")\n",
        "  \n",
        "  else:\n",
        "    print(\"\\nTweet relevant to a disaster\\n\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a tweet to see if it's relevant to a disaster: \n",
            "manage large analytics data sets in moden efficient fashion with #ONTAPAI and #Omnisci\n",
            "\n",
            "Tweet not relevant to a disaster\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}